{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c88936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aceedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "model_name=\"/home/ec2-user/SageMaker/GPTJ/amazon-sagemaker-gptj/finetune/\"\n",
    "\n",
    "#local_rank = int(os.getenv('LOCAL_RANK', '7'))\n",
    "#world_size = int(os.getenv('WORLD_SIZE', '8'))\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0,torch_dtype=torch.float16)\n",
    "\n",
    "generator.model = deepspeed.init_inference(\n",
    "                                        generator.model,\n",
    "                                        tensor_parallel={\"tp_size\": 1},\n",
    "                                        dtype=torch.half,\n",
    "                                        replace_method='auto',\n",
    "                                        max_tokens=2048,\n",
    "                                        replace_with_kernel_inject=True\n",
    "                                          \n",
    "                                          )\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e62872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model is loaded on device {generator.model.module.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_text = \"love: \"\n",
    "string = generator(input_text, do_sample=True, max_length=2047,min_length=2047, top_k=50, top_p=0.95, temperature=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0348ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f031ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-m',\"--model_name\", type=str, default='EleutherAI/gpt-j-6B')\n",
    "    parser.add_argument('--local_rank', type=int, default=-1,\n",
    "                    help='local rank passed from distributed launcher')\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "    args = parser.parse_args()\n",
    "    model_name = args.model_name\n",
    "\n",
    "    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "    world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=local_rank,torch_dtype=torch.float16)\n",
    "\n",
    "    generator.model = deepspeed.init_inference(generator.model,\n",
    "                                            mp_size=world_size,\n",
    "                                            dtype=torch.half,\n",
    "                                            replace_method='auto',\n",
    "                                            max_tokens=2048,\n",
    "                        replace_with_kernel_inject=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    input_text = \"DeepSpeed is\"\n",
    "    string = generator(input_text, do_sample=True, max_length=2047,min_length=2047, top_k=50, top_p=0.95, temperature=0.9)\n",
    "\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71374a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4775b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name=\"/home/ec2-user/SageMaker/GPTJ/amazon-sagemaker-gptj/finetune/\"\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
