{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786046cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab20556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd69cbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-02-10 19:33:53,790] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\n",
      "[2023-02-10 19:33:53,792] [INFO] [logging.py:68:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu116 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py39_cu116/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.48761963844299316 seconds\n",
      "[2023-02-10 19:33:54,999] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 16384, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 64, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': False, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 2048, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False}\n",
      "Using /home/ec2-user/.cache/torch_extensions/py39_cu116 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.06675481796264648 seconds\n",
      "CPU times: user 40min 58s, sys: 1min 41s, total: 42min 40s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "model_name=\"/home/ec2-user/SageMaker/GPTJ/amazon-sagemaker-gptj/finetune/\"\n",
    "\n",
    "#local_rank = int(os.getenv('LOCAL_RANK', '7'))\n",
    "#world_size = int(os.getenv('WORLD_SIZE', '8'))\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0,torch_dtype=torch.float16)\n",
    "\n",
    "generator.model = deepspeed.init_inference(\n",
    "                                        generator.model,\n",
    "                                        tensor_parallel={\"tp_size\": 1},\n",
    "                                        dtype=torch.half,\n",
    "                                        replace_method='auto',\n",
    "                                        max_tokens=2048,\n",
    "                                        replace_with_kernel_inject=True\n",
    "                                          \n",
    "                                          )\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model is loaded on device {generator.model.module.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ba6eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 0 ns, total: 14.9 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_text = \"love: \"\n",
    "string = generator(input_text, do_sample=True, max_length=1000,min_length=300, top_k=50, top_p=0.95, temperature=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebd2bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'love:  We must learn to put aside our pride of personality and recognize our common ground as human beings. Our greatest need is the love that will transform us into our common ground, and then that love will bring us together as a people. Only then will we be able to live together in peace. We must stop the constant war. We must stop the endless bickering. We must stop the constant striving for the top. We must stop the constant blaming. And we must finally accept one another for our differences and love each other. That is the hope of the future and the promise of our eternal welfare. That is the hope of life for all of us. That is our hope. That is our promise. That is the hope of humanity. That is the promise of life. That is the hope of the future. That is the promise of life. That is the hope of the future. That is the promise of life. Let us keep these promises. Let us be worthy of the hopes of the past. Let us be worthy of the hopes of the future. Let us work together for the sake of peace and prosperity and for all that is best in us. And let us be true to those promises. And let us always keep the best thought of the future in our minds. That is the hope of life. That is the promise of life. That is the hope of the future. That is the promise of life. That is the hope of the future. That is the promise'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e5233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-m',\"--model_name\", type=str, default='EleutherAI/gpt-j-6B')\n",
    "    parser.add_argument('--local_rank', type=int, default=-1,\n",
    "                    help='local rank passed from distributed launcher')\n",
    "    parser = deepspeed.add_config_arguments(parser)\n",
    "    args = parser.parse_args()\n",
    "    model_name = args.model_name\n",
    "\n",
    "    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "    world_size = int(os.getenv('WORLD_SIZE', '1'))\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=local_rank,torch_dtype=torch.float16)\n",
    "\n",
    "    generator.model = deepspeed.init_inference(generator.model,\n",
    "                                            mp_size=world_size,\n",
    "                                            dtype=torch.half,\n",
    "                                            replace_method='auto',\n",
    "                                            max_tokens=2048,\n",
    "                        replace_with_kernel_inject=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    input_text = \"DeepSpeed is\"\n",
    "    string = generator(input_text, do_sample=True, max_length=2047,min_length=2047, top_k=50, top_p=0.95, temperature=0.9)\n",
    "\n",
    "    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7374b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name=\"/home/ec2-user/SageMaker/GPTJ/amazon-sagemaker-gptj/finetune/\"\n",
    "\n",
    "model = GPTJForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
