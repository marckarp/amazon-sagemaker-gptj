{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune GPTJ on SageMaker Training with Deepspeed\n",
    "\n",
    "In this notebook we will finetune GPTJ on the processed [quotes dataset](https://www.kaggle.com/datasets/akmittal/quotes-dataset). This notebook/repo makes use of [this GitHub repo](https://github.com/Xirider/finetune-gpt2xl) where the Dockerfile has been adpated to be compliant with SageMaker. \n",
    "\n",
    "The dataset is in the format we wish to make inference with:\n",
    "\n",
    "`<Catagory>: <AIGeneratedQuote>`+\n",
    "\n",
    "Example: \n",
    "*We want to give GTJ a catagory and it must generate a quote*\n",
    "\n",
    "`love: <AIGeneratedQuote>`\n",
    "\n",
    "Take a look at the quote dataset we will be using in this notebook. \n",
    "1. [train.csv](https://raw.githubusercontent.com/marckarp/amazon-sagemaker-fine-tune-gptj/main/Finetune_GPTNEO_GPTJ6B/quotes_dataset/train.csv)\n",
    "2. [validation.csv](https://raw.githubusercontent.com/marckarp/amazon-sagemaker-fine-tune-gptj/main/Finetune_GPTNEO_GPTJ6B/quotes_dataset/validation.csv)\n",
    "\n",
    "If you wish to make use of your own dataset feel free to create a train and validation dataset with your own data to accomplish the task you are setting out to achieve. \n",
    "\n",
    "For more details on preparing a dataset please see [this link](https://github.com/mallorbc/Finetune_GPTNEO_GPTJ6B/tree/main/finetuning_repo#preparing-a-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Push the container for SageMaker Training\n",
    "\n",
    "In order to fine tune GPTJ we will have to make use of a docker container with Deepspeed installed. \n",
    "The Dockerfile is adapted from this repo [here](https://github.com/mallorbc/Finetune_GPTNEO_GPTJ6B/blob/main/Dockerfile). It has been adapted to be SageMaker compatible. \n",
    "\n",
    "Below we will define the deepspeed CLI command that will be run within our SageMaker Training Job. It has been paramterized using Enviroment variables so that we can have the ability to tune/customize the parameters when we kick of a SageMaker Training Job. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../Finetune_GPTNEO_GPTJ6B/train\n",
    "#!/bin/bash\n",
    "\n",
    "df -h\n",
    "cd finetuning_repo\n",
    "\n",
    "deepspeed --num_gpus=$num_gpus run_clm.py --deepspeed $deepspeed --model_name_or_path EleutherAI/gpt-j-6B --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/validation/validation.csv --do_train --do_eval --fp16 --overwrite_cache --evaluation_strategy=$evaluation_strategy --output_dir $output_dir --num_train_epochs $num_train_epochs  --eval_steps $eval_steps --gradient_accumulation_steps $gradient_accumulation_steps --per_device_train_batch_size $per_device_train_batch_size --use_fast_tokenizer $use_fast_tokenizer --learning_rate $learning_rate --warmup_steps $warmup_steps --save_total_limit $save_total_limit --save_steps $save_steps --save_strategy $save_strategy --tokenizer_name $tokenizer_name --load_best_model_at_end=$load_best_model_at_end --block_size=$block_size --weight_decay=$weight_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the train file we created above as executable. Once we have all our files ready we can build and push our image to ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "\n",
    "cd ../Finetune_GPTNEO_GPTJ6B\n",
    "chmod +x train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd ../Finetune_GPTNEO_GPTJ6B\n",
    "./build_push_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training\n",
    "\n",
    "Once the image has been pushed to ECR we can then kick off the Training Job but first we need to create a SageMaker Estimator object. That contains the information required to start a Training Job. \n",
    "\n",
    "There is a number of paramaters you can tune. Depending on the number of GPUs avaialble you set `num_gpus`. The deepspeed configratuon file is also parameterized. \n",
    "\n",
    "There are three options to choose from:\n",
    "1. ds_config_stage1.json\n",
    "2. ds_config_stage2.json\n",
    "3. ds_config_stage3.json\n",
    "\n",
    "https://github.com/mallorbc/Finetune_GPTNEO_GPTJ6B/tree/main/finetuning_repo#deepspeed\n",
    "\n",
    "\n",
    "Training and finetuning a model is an experimental science. You may want to tune different learning rates, weight decay, etc.\n",
    "\n",
    "The Training Job has also been configured to emit metrics such as `eval_loss`. The regex for the metrics that the container emits is specified in `metric_definitions`.  You can use these metrics to decide if you wish to stop the Training Job if you do not see an improvement in the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker \n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "from sagemaker import local\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#local_sagemaker_session = local.LocalSession()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "account = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/gptj-finetune:latest'.format(account, region)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket() # Set a default S3 bucket\n",
    "prefix = 'DEMO-fine-tune-GPTJ'\n",
    "\n",
    "\n",
    "sm_model = sagemaker.estimator.Estimator(\n",
    "image_uri=image,\n",
    "role=role,\n",
    "instance_count = 1,\n",
    "#instance_type='local_gpu', \n",
    "#sagemaker_session=local_sagemaker_session,\n",
    "sagemaker_session= sagemaker_session,\n",
    "#instance_type = 'ml.g5.48xlarge',\n",
    "instance_type=\"ml.g5.12xlarge\",\n",
    "environment = {\n",
    "    \"num_gpus\": \"4\",\n",
    "    \"deepspeed\": \"ds_config_stage3.json\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"output_dir\": \"/opt/ml/checkpoints/\",\n",
    "    \"num_train_epochs\": \"12\",\n",
    "    \"eval_steps\": \"20\",\n",
    "    \"gradient_accumulation_steps\": \"1\",\n",
    "    \"per_device_train_batch_size\": \"4\",\n",
    "    \"use_fast_tokenizer\": \"False\",\n",
    "    \"learning_rate\": \"5e-06\",\n",
    "    \"warmup_steps\": \"10\",\n",
    "    \"save_total_limit\": \"1\",\n",
    "    \"save_steps\": \"20\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"tokenizer_name\": \"gpt2\",\n",
    "    \"load_best_model_at_end\": \"True\",\n",
    "    \"block_size\": \"2048\",\n",
    "    \"weight_decay\": \"0.1\"\n",
    "},\n",
    "checkpoint_s3_uri=f\"s3://{bucket}/fine-tune-GPTJ/first-run/checkpoint/\",\n",
    "    \n",
    "output_path=f\"s3://{bucket}/fine-tune-GPTJ/first-run/\",\n",
    "\n",
    "metric_definitions=[\n",
    "    {'Name': 'eval:loss', 'Regex': \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {'Name': 'eval:runtime', 'Regex': \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {'Name': 'eval:samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {'Name': 'eval:eval_steps_per_second', 'Regex': \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we will be using the processed quotes dataset to finetune GPTJ and as such we upload the train and validation set to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../Finetune_GPTNEO_GPTJ6B/quotes_dataset/train.csv to s3://sagemaker-us-east-1-171503325295/fine-tune-GPTJ/datasets/train/train.csv\n",
      "upload: ../Finetune_GPTNEO_GPTJ6B/quotes_dataset/validation.csv to s3://sagemaker-us-east-1-171503325295/fine-tune-GPTJ/datasets/validation/validation.csv\n"
     ]
    }
   ],
   "source": [
    "train_s3=f\"s3://{bucket}/fine-tune-GPTJ/datasets/train/train.csv\"\n",
    "val_s3=f\"s3://{bucket}/fine-tune-GPTJ/datasets/validation/validation.csv\"\n",
    "\n",
    "\n",
    "!aws s3 cp ../Finetune_GPTNEO_GPTJ6B/quotes_dataset/train.csv $train_s3\n",
    "!aws s3 cp ../Finetune_GPTNEO_GPTJ6B/quotes_dataset/validation.csv $val_s3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The container we bult expects two datasets, a train and validation dataset. Here set the training input channels \"train\" and \"validation\" which each point to their respective S3 locations for SageMaker to make use of during Training. SageMaker handles downloading the datasets to the container from S3 on our behalf. \n",
    "\n",
    "Finally, we kick off the job with the `.fit()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: gptj-finetune-2023-02-10-20-31-13-476\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    train_s3, content_type=\"csv\"\n",
    ")\n",
    "validation_input = TrainingInput(\n",
    "    val_s3, content_type=\"csv\"\n",
    ")\n",
    "\n",
    "sm_model.fit({\"train\": train_input, \"validation\": validation_input}, wait=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE checkpoint-120/\n"
     ]
    }
   ],
   "source": [
    "s3_checkpoints = f\"s3://{bucket}/fine-tune-GPTJ/checkpoint/\"\n",
    "!aws s3 ls $s3_checkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
